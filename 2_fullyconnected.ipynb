{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "mainRoot = '/home/christophe/data/'\n",
    "\n",
    "pickle_file = mainRoot + 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random valued following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable( tf.truncated_normal([image_size * image_size, num_labels]) )\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) )\n",
    "\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax( tf.matmul(tf_valid_dataset, weights) + biases )\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 15.548222\n",
      "Training accuracy: 12.5%\n",
      "Validation accuracy: 16.2%\n",
      "Loss at step 100: 2.315109\n",
      "Training accuracy: 71.8%\n",
      "Validation accuracy: 71.1%\n",
      "Loss at step 200: 1.887378\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 73.5%\n",
      "Loss at step 300: 1.642345\n",
      "Training accuracy: 75.8%\n",
      "Validation accuracy: 74.5%\n",
      "Loss at step 400: 1.469720\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 74.9%\n",
      "Loss at step 500: 1.339245\n",
      "Training accuracy: 77.5%\n",
      "Validation accuracy: 75.3%\n",
      "Loss at step 600: 1.237039\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 75.7%\n",
      "Loss at step 700: 1.154888\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 76.0%\n",
      "Loss at step 800: 1.087461\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 75.9%\n",
      "Test accuracy: 83.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the biases.\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(predictions, train_labels[:train_subset, :]))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph dependencies.\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) # ONLY DIFF FOR SGD\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) # ONLY DIFF FOR SGD\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.447895\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 8.2%\n",
      "Minibatch loss at step 500: 1.651610\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 1000: 1.589200\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 1500: 1.427056\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 2000: 0.816463\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 2500: 0.982241\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 3000: 0.740258\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 79.4%\n",
      "Test accuracy: 86.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 316.675568\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 37.2%\n",
      "Minibatch loss at step 100: 15.266980\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 200: 16.117695\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 300: 15.775345\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 400: 14.774477\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 500: 26.008680\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 600: 5.998812\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 700: 16.808561\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 800: 11.274008\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 900: 23.323954\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1000: 12.198814\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1100: 9.507034\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1200: 5.757495\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 1300: 8.723159\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 1400: 7.950072\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1500: 11.485847\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1600: 8.035740\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 1700: 8.423284\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1800: 4.159997\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1900: 8.386874\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2000: 5.061366\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 2100: 7.089533\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2200: 2.530754\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2300: 4.520854\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 2400: 3.748410\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 2500: 7.128826\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2600: 3.692081\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 2700: 3.077800\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 2800: 6.631020\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 2900: 2.773773\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 3000: 3.434244\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.1%\n",
      "Test accuracy: 90.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEZCAYAAACD/A7qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd33b0a3748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) # ONLY DIFF FOR SGD\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) # ONLY DIFF FOR SGD\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    def forward_prop(inp):\n",
    "        h1 = tf.nn.relu(tf.matmul(inp, weights_1) + biases_1)\n",
    "        return tf.matmul(h1,weights_2) + biases_2\n",
    "\n",
    "    # Training computation.\n",
    "    logits = forward_prop(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(forward_prop(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(forward_prop(tf_test_dataset))\n",
    "\n",
    "\n",
    "num_steps = 3001\n",
    "data = np.ndarray(shape=(1+num_steps//100,4), dtype=np.float32)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 100 == 0):\n",
    "            batch_score = accuracy(predictions, batch_labels)\n",
    "            valid_score = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            test_score = accuracy(test_prediction.eval(), test_labels)\n",
    "            data[step//100,:] = [step*batch_size, batch_score/100, valid_score/100, test_score/100]\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % batch_score)\n",
    "            print(\"Validation accuracy: %.1f%%\" % valid_score)\n",
    "    print(\"Test accuracy: %.1f%%\" % test_score)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(data[:,0], data[:,1:4])\n",
    "ax.set_title('Scores given training size')\n",
    "ax.legend(('batch','valid', 'test'), loc='lower right')\n",
    "ax.set_xticks(data[:,0])\n",
    "ax.set_xlabel('Training size')\n",
    "ax.set_ylim(0,1.01)\n",
    "ax.grid()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
